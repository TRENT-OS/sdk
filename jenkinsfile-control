//------------------------------------------------------------------------------
//
// SDK Jenkins Pipeline Control Script
//
// Copyright (C) 2020-2021, HENSOLDT Cyber GmbH
//
//------------------------------------------------------------------------------


def SDK_PACKAGE = 'sdk-package.tar.bz2'

//------------------------------------------------------------------------------
// System context
class SystemCtx {

    static FLAG_BUILD      = 1 << 0
    static FLAG_TEST       = 1 << 1

    String      systemName   // Name of the system to build.
    Integer     flags        // Flags
    String      repo         // Repo location of the system.
    ArrayList   platforms    // List of supported platforms.
    String      testScript   // Test script to be executed.
    String      buildParams  // Additional build parameters.

    //--------------------------------------------------------------------------
    // Format of 'systemName' is "name[@path/to/repo]". If no repo is given, we
    // use "ss/<systemName>". There is no ".git" extension given here here, this
    // will be added internally.
    public SystemCtx(
        String systemName,
        Integer flags,
        ArrayList platforms,
        String testScript,
        String buildParams)
    {
        // Use ss/<systemName> as default repo.
        def repo = 'ss/' + systemName

        // Split <systemName> to detect non-default repo.
        def arr = systemName.split('@',2)
        if (2 == arr.length)
        {
            systemName = arr[0]
            repo = arr[1]
        }

        this.systemName  = systemName
        this.flags       = flags
        this.repo        = repo
        this.platforms   = platforms
        this.testScript  = testScript
        this.buildParams = buildParams
    }
}

//------------------------------------------------------------------------------
def BuildCtx(
    String systemName,
    ArrayList platforms = null,
    String buildParams = null)
{
    return new SystemCtx(
                    systemName,
                    SystemCtx.FLAG_BUILD,
                    platforms,
                    null, // obviously, there is no testScript
                    buildParams)
}

//------------------------------------------------------------------------------
def TestCtx(
        String systemName,
        ArrayList platforms = null,
        String testScript = null,  // use <systemName>.py if null
        String buildParams = null)
{
    return new SystemCtx(
                    systemName,
                    SystemCtx.FLAG_BUILD | SystemCtx.FLAG_TEST,
                    platforms,
                    testScript ?: systemName + '.py',
                    buildParams)
}


//------------------------------------------------------------------------------
// Build/Test configurations to be executed in the generic pipeline.
//
def SYSTEM_CONFIGS = [

    // The "Hello World" demo also acts as a test for the CI feature to use a
    // verbose repo location of the form 'name@path/to/repo'.
    TestCtx('demo_hello_world@ss/demo_hello_world', ['zynq7000'], 'test_demo_hello_world.py'),
    TestCtx('demo_hello_world',       ['sabre'], 'test_demo_hello_world.py'),
    BuildCtx('demo_hello_world',      ['rpi3']),

    // Demo systems
    TestCtx('demo_iot_app',           ['zynq7000'], 'test_demo_iot_app.py'),
    BuildCtx('demo_iot_app_rpi3',     ['rpi3']),
    TestCtx('demo_tls_api',           ['zynq7000'], 'test_demo_tls_api.py'),
    BuildCtx('demo_network_filter',   ['zynq7000']),
    BuildCtx('demo_raspi_ethernet',   ['rpi3']),
    BuildCtx('demo_i2c',              ['sabre']),

    // Test systems
    TestCtx('test_timeserver',        ['zynq7000', 'sabre']),
    BuildCtx('test_timeserver',       ['rpi3']),
    TestCtx('test_certserver',        ['zynq7000', 'sabre']),
    BuildCtx('test_certserver',       ['rpi3']),
    TestCtx('test_uart',              ['zynq7000', 'sabre']),
    TestCtx('test_chanmux',           ['zynq7000']),
    TestCtx('test_proxy_nvm',         ['zynq7000']),
    TestCtx('test_certparser',        ['zynq7000']),
    TestCtx('test_crypto_api',        ['zynq7000']),
    TestCtx('test_cryptoserver',      ['zynq7000']),
    TestCtx('test_entropysource',     ['zynq7000']),
    TestCtx('test_storage_interface', ['zynq7000', 'sabre'], 'test_storage_interface*.py'),
    TestCtx('test_filesystem',        ['zynq7000']),
    TestCtx('test_config_server',     ['zynq7000']),
    TestCtx('test_keystore',          ['zynq7000']),
    TestCtx('test_logserver',         ['zynq7000']),
    TestCtx('test_network_api',       ['zynq7000']),
    TestCtx('test_tls_api',           ['zynq7000']),
    TestCtx('test_tlsserver',         ['zynq7000'])

]


//------------------------------------------------------------------------------
// Docker
//
// Notes:
// * bind the localtime to docker container to avoid problems of gaps between
//   the localtime of the container and the host.
// * add user to group "stack" (1001) in order to grant usage of Haskell stack
//   in the docker image
//
// ToDo
// * why are sudo right needed in the test container
//

def DOCKER_BASE = 'docker:5000'
def DOCKER_REGISTRY = 'https://' + DOCKER_BASE

def DOCKER_BUILD_ENV = [
    registry: DOCKER_REGISTRY,
    image:    DOCKER_BASE + '/trentos_build:20210503',
    args:     ['-v /etc/localtime:/etc/localtime:ro',
               '--group-add=stack',
               '--group-add=sudo'
              ].join(' ')
]

def DOCKER_TEST_ENV  = [
    registry: DOCKER_REGISTRY,
    image:    DOCKER_BASE + '/trentos_test:20210303',
    args:     ['-v /home/jenkins/.ssh/:/home/jenkins/.ssh:ro',
               '-v /etc/localtime:/etc/localtime:ro',
               '--network=bridge',
               '--cap-add=NET_ADMIN',
               '--cap-add=NET_RAW',
               '--device=/dev/net/tun',
               '--group-add=sudo'
              ].join(' ')
]



//------------------------------------------------------------------------------
def print_step_info(name)
{
    println('######## ' + name)
}


//------------------------------------------------------------------------------
def do_notify_bitbucket()
{
    // the StashNotifier requires to run in a node block, which should not be
    // necessary technically. See also the open issue issue at
    // https://github.com/jenkinsci/stashnotifier-plugin/issues/234

    if (env.NODE_NAME)
    {
        notifyBitbucket()
    }
    else
    {
        // If we don't specify any node here explicitly, it will not run on the
        // master node even if it is free, because the master is reserved
        // exclusively for bound jobs. If all other executors are busy, the
        // whole job will stuck just for sending a notification. Thus we use the
        // master node for the notification.
        node('pipeline-control') { notifyBitbucket() }
    }
}


//------------------------------------------------------------------------------
def do_stash(name, file_list)
{
    println('stashing to ' + name)
    stash([
        name: name,
        includes: file_list.join(',')
    ])
}


//------------------------------------------------------------------------------
def do_archiveArtifacts(
    artifacts,
    fingerprint = true
) {
    archiveArtifacts([
        artifacts: artifacts,
        fingerprint: fingerprint
    ])

    manager.createSummary("package.png").appendText(
        '<hr>'+
        'adding Artifact(s): <code><strong>' + artifacts + '</strong></code> '+
        '(see <a href="artifact">artifact/</a>)'
    )
}


//------------------------------------------------------------------------------
def save_current_job_log(
    name
) {
    // get the current job's log up to now. One day we should find a nicer way
    // for this than downloading it

    run_app(
        'wget',
        [
            '-q',
            '--no-check-certificate',
            '-O ' + name,
            BUILD_URL + 'consoleText'
        ]
    )
}


//------------------------------------------------------------------------------
def get_build_relative_url(build)
{
    // getAbsoluteUrl() is deprecated and generally regarded as a bad idea for
    // generating HTML page content. This workaround avoids accessing the raw
    // build, which can't be done by scripts by default

    return '/job/' +
            build.getFullProjectName().replace("/", "/job/") +
            '/' +
            build.getNumber()

}


//------------------------------------------------------------------------------
def get_result_ball_img_url(result)
{
    def states = [
        'SUCCESS':  'green.gif', // there is no png
        'UNSTABLE': 'yellow.png',
        'FAILURE':  'red.png',
        'ABORTED':  'aborted.png'
    ]

    return Jenkins.RESOURCE_PATH +
            '/images/16x16/' +
            states.get(result, 'help.png')
}


//------------------------------------------------------------------------------
def run_app(
    application,
    param_array = null
) {
    def cmdLine = application

    if (param_array)
    {
        cmdLine += ' ' + param_array.join(' ')
    }

    sh(cmdLine)
}


//------------------------------------------------------------------------------
def run_shell_script(
    script,
    param_array = null
) {
    run_app(script, param_array)
}


//------------------------------------------------------------------------------
def do_git_checkout(
    repo,
    branch,
    folder = NULL
) {
    println(
        'checkout ' + repo + '@' + branch +
        ( folder ? (' into ' + folder) : '' ) +
        ' ...'
    )

    try
    {
        def scmVars = checkout([
                    poll: false,
                    scm: [
                        $class: 'GitSCM',
                        branches: [
                            [name: branch]
                        ],
                        doGenerateSubmoduleConfigurations: false,
                        extensions: [
                            [
                                $class: 'RelativeTargetDirectory',
                                relativeTargetDir: (folder ? folder : '.')
                            ],

                            [
                                $class: 'SubmoduleOption',
                                disableSubmodules: false,
                                recursiveSubmodules: true,
                                trackingSubmodules: false,
                                parentCredentials: true,
                                // parallel checkouts speed up things, but too
                                // many threads (from too man parallel jobs)
                                // will eventually overload the GIT server and
                                // we get failures due to timeouts. We've seen
                                // this happening with 8 threads, so try 4 now.
                                threads: 4
                            ]
                        ],
                        userRemoteConfigs: [
                            [
                                credentialsId: 'ac3a90b4-fd3f-4aa4-b28e-bd78878588ad',
                                url: 'ssh://git@git-server:7999/' + repo + '.git'
                            ]
                        ]
                    ]
                ])

        // clean up some strange folders jenkins creates for an unknown reason
        if (folder) { run_app('rmdir', [folder+'@tmp']) }

        return scmVars
    }
    catch (Exception e)
    {
        println 'Exception: ' + e
        return null
    }
}


//------------------------------------------------------------------------------
def do_checkout(
    repo,
    branch,
    dir = '.'
) {
    def summary = manager.createSummary("document.png")
    summary.appendText('<hr>Repo: <code><strong>' + repo + '</strong></code>')

    def scmVars = do_git_checkout(repo, branch, dir)

    // if this is a custom branch, then fall back to integration
    if ( (!scmVars) && ( !(branch in ['integration', 'master']) ) )
    {
        def fallback_branch = 'integration'
        summary.appendText(
            ' (using <code><strong>' + fallback_branch + '</strong></code>, '+
            'no branch <code><strong>' + branch + '</strong></code>)'
        )
        scmVars = do_git_checkout(repo, fallback_branch, dir)
    }

    if (!scmVars)
    {
        echo 'checkout error'
        return
    }
}


//------------------------------------------------------------------------------
def execute_build(ctx, stage_ctx)
{
    def jobBuild = build(
        job: 'generic_jobs/generic_pipeline_sandbox',
        wait: true,   // block call until finished
        propagate: false, // don't throw exception on error
        parameters: [
            string(name: 'PLATFORM',           value: stage_ctx.platform ?: ''),
            string(name: 'BRANCH_OR_COMMIT',   value: ctx.branch),
            string(name: 'TEST_SYSTEM',        value: stage_ctx.test_ctx.repo),
            string(name: 'BUILD_PARAMS',       value: stage_ctx.test_ctx.buildParams ?: ''),
            string(name: 'TEST_SCRIPT',        value: stage_ctx.test_ctx.testScript ?: ''),
            string(name: 'UPSTREAM_JOB_NAME',  value: currentBuild.fullProjectName),
            string(name: 'UPSTREAM_JOB_ID',    value: currentBuild.id),
        ]
    )

    // if we arrive here, the job has finished
    stage_ctx.build = jobBuild
    def result = jobBuild.getResult()
    def duration = jobBuild.getDurationString()
    def cnt = ctx.jobStages.size()

    lock('job-post-processing')
    {
        def num = cnt - (--ctx.running)

        println(num + '/' + cnt + ': ' +
                stage_ctx.test_ctx.systemName + ' system build/test stage took '
                + duration + ', result: ' + result)

        def flags = stage_ctx.test_ctx.flags
        def operations = []
        if (flags & SystemCtx.FLAG_BUILD) { operations += ['build'] }
        if (flags & SystemCtx.FLAG_TEST) { operations += ['test'] }

        ctx.summary.appendText(
            stage_ctx.platform + ', ' + stage_ctx.test_ctx.systemName + ':' +
            ' <a href="' + get_build_relative_url(jobBuild) + '">' +
                '#' + jobBuild.getNumber() +
            '</a>' +
            ' <img src="' + get_result_ball_img_url(result) + '">' +
            (operations ? (' ' + operations.join(', ')) : '') +
            ' (' + duration + ')' +
            '<br>',
            false)
    }

    // When CI is under heavy load, no free executor could be available to run
    // the artifact copy from the child job. In the worst case the child job
    // already got deleted. We could extend the time before the job is deleted,
    // but this may overload CI also because the workspaces eat the disk space.
    // Better solution is running the copy job on a dedicated executor, which
    // exists on the master
    node('pipeline-control')
    {
        cleanWs()

        catchError(buildResult: 'UNSTABLE', stageResult: 'FAILURE')
        {
            copyArtifacts(
                filter: stage_ctx.system_pkg,
                projectName: jobBuild.getFullProjectName(),
                // need to ensure this is a string
                selector: specific("${jobBuild.getNumber()}")
            )

            do_stash(
                stage_ctx.stash_name,
                [
                    stage_ctx.system_pkg
                ]
            )
        }

        cleanWs()
    }

    // We have set propagate=false above, so we have to deal with the failures
    // here. Since all this runs in block wrapped by catchError(), we can't
    // really fail the build here, but we can ensure a message is logged and
    // the state is set properly.
    if ('SUCCESS' != result)
    {
        def msg = 'test ' + result + ' for ' + stage_ctx.platform + ', ' +
                  stage_ctx.test_ctx.systemName

        if ('UNSTABLE' == result) {
            // propagate unstable
            unstable(msg)
        }
        else {
            // 'FAILURE', 'ABORTED' is fatal
            error(msg)
        }
    }
}


//------------------------------------------------------------------------------
def do_jobs_for_test_systems(branch, testConfigs)
{
    // downstream job create this package and finally we also pack all the
    // collected results into a package with this name
    def SYSTEM_PACKAGE = 'package.bz2'

    def summary = manager.createSummary("orange-square.png")
    summary.appendText('<hr>')

    def sub_jobs_ctx = [
        summary:   summary,
        branch:    branch,
        jobStages: [:],
        running:   0
    ]

    // if we use a loop "for (test_ctx in testConfigs) ..." then only one
    // instance of the variable test_ctx exists. When the closures are invoked
    // after the loop, they all see the last value that the loop variable. When
    // a loop "testConfigs.each { test_ctx -> ..." is used, a closure with
    // a separate variable is create for each iteration, which can be used in
    // further closures then also independently.

    testConfigs.each { test_ctx ->

        def test_system = test_ctx.systemName

        test_ctx.platforms.each { platform ->

            def pkg_name = platform + '-' + test_system

            def job_stage_ctx = [
                // do not try storing "sub_jobs_ctx" in here, this ends in a
                // strange recursion when transforming the maps/lists
                test_ctx:     test_ctx,
                platform:     platform,
                system_pkg:   SYSTEM_PACKAGE,
                stash_name:   pkg_name,
                closure:      null,   // set below
                build:        null    // set in closure
            ]

            sub_jobs_ctx.jobStages[pkg_name] = job_stage_ctx

            job_stage_ctx.closure = {
                stage(pkg_name) {
                    execute_build(sub_jobs_ctx, job_stage_ctx)
                }
            }
        }
    }


    sub_jobs_ctx.running  = sub_jobs_ctx.jobStages.size()
    def build_start = new Date()

    catchError(buildResult: 'UNSTABLE', stageResult: 'FAILURE')
    {
        // we don't need an executor to start the jobs and wait for them to
        // finish
        //
        //  parallel
        //      firstBranch:  { /* do something */ },
        //      secondBranch: { /* do something */ },
        //      failFast: true|false // terminate all upon any one failing

        parallel(
            sub_jobs_ctx.jobStages.collectEntries { k, v -> [(k): v.closure] }
        )
    }

    duration = groovy.time.TimeCategory.minus(new Date(), build_start)
    def duration_msg = 'all system build/test stages took ' + duration
    println(duration_msg)
    summary.appendText('<hr>' + duration_msg, false)

    // builds are done, now collect the results and artifacts. This needs a
    // workspace, so we have to allocate a node

    def cnt = sub_jobs_ctx.jobStages.size()
    def was_build_error = false

    node {
        stage('process_results') {

            cleanWs()

            sub_jobs_ctx.jobStages.eachWithIndex { e, idx ->

                def pkg_name = e.key
                def job_stage_ctx = e.value

                println('processing ' + (idx+1) + '/' + cnt + ': ' + pkg_name)

                // there might be no stash if something failed earlier
                catchError(buildResult: 'UNSTABLE', stageResult: 'FAILURE')
                {
                    unstash(pkg_name)
                    def pkg_dir = 'package-' + pkg_name
                    run_app('mkdir', [pkg_dir])
                    run_app('tar', ['-xf ' + SYSTEM_PACKAGE, '-C ' + pkg_dir])
                    run_app('rm', [SYSTEM_PACKAGE])
                }

                def pkg_build = job_stage_ctx.build
                was_build_error |= (!pkg_build) ||
                                   (pkg_build.getResult() in ['FAILURE','ABORTED'])
            }

            // sometimes this job runs just to test the SDK generation, so
            // there are no test systems enabled. We don't want to see things
            // fail with junit complaining about missing test files in this
            // case.
            if (cnt > 0)
            {
                // ToDo: support multiple platforms here, currently this will
                //       see the same test case for each platform and it's
                //       likely messing up the analysis
                junit('package-*/test_results.xml')
            }

            save_current_job_log('build.log')

            // create a package of all files, so the next stage can process
            // them. Ensure we never fail here if a file is missing, because
            // there is not much we can do anyway. The next stage is supposed
            // to handle this.
            run_app(
                'tar',
                [
                    '--ignore-failed-read',
                    '-cjf '+SYSTEM_PACKAGE,
                    'build.log',
                    'package-*/'
                ]
            )

            do_archiveArtifacts(SYSTEM_PACKAGE)

            cleanWs()

        }
    }

    if (was_build_error)
    {
        error('stage failed: run test systems')
    }
}


//------------------------------------------------------------------------------
pipeline {

    agent none

    options {

        skipDefaultCheckout()

        // disableConcurrentBuilds()

        // unfortunately there is no way to have build discarding conditional
        // for development branches only, so "master" and "integration" would
        // keep their builds. The potential work around is having a conditional
        // stage where "master" and "integration" push a package to an artifact
        // server, which then preserves this.
        buildDiscarder(logRotator(numToKeepStr: '20'))
    }

    stages {

        //----------------------------------------------------------------------
        stage('Build SDK') {

            agent { label "build" }

            stages {

                //--------------------------------------------------------------
                stage('checkout') {
                    steps {
                        print_step_info env.STAGE_NAME
                        cleanWs()
                        checkout([
                            $class: 'GitSCM',
                            branches: scm.branches,
                            doGenerateSubmoduleConfigurations: false,
                            extensions: [
                                [
                                    $class: 'RelativeTargetDirectory',
                                    relativeTargetDir: 'scm-src'
                                ],
                                [
                                    $class: 'SubmoduleOption',
                                    disableSubmodules: false,
                                    recursiveSubmodules: true,
                                    trackingSubmodules: false,
                                    parentCredentials: true,
                                    threads: 4,
                                ]
                            ],
                            userRemoteConfigs: scm.userRemoteConfigs
                        ])

                        do_notify_bitbucket()
                    }
                }

                //--------------------------------------------------------------
                stage('package') {
                    agent {
                        docker {
                            reuseNode true
                            alwaysPull true
                            registryUrl DOCKER_BUILD_ENV.registry
                            image DOCKER_BUILD_ENV.image
                            args DOCKER_BUILD_ENV.args
                        }
                    }
                    steps {
                        print_step_info env.STAGE_NAME
                        run_shell_script(
                            'scm-src/build-sdk.sh',
                            [
                                'package',    // mode
                                'sdk-package' // SDK package output base dir
                            ]
                        )
                    }
                }

                //--------------------------------------------------------------
                stage('archive') {
                    steps {
                        print_step_info env.STAGE_NAME

                        do_archiveArtifacts(SDK_PACKAGE)

                        do_stash(
                            'stash_doc',
                            [
                                'scm-src/publish_doc.sh',
                                'sdk-package/pkg/doc/',
                            ]
                        )
                    }
                }

                //--------------------------------------------------------------
                stage('unit_test') {
                    agent {
                        docker {
                            reuseNode true
                            alwaysPull true
                            registryUrl DOCKER_TEST_ENV.registry
                            image DOCKER_TEST_ENV.image
                            args DOCKER_TEST_ENV.args
                        }
                    }
                    steps {
                        print_step_info env.STAGE_NAME
                        catchError(buildResult: 'UNSTABLE', stageResult: 'FAILURE') {
                            run_shell_script(
                                'scm-src/build-sdk.sh',
                                [
                                    'unit-tests', // mode
                                    'sdk-package' // base dir
                                ]
                            )
                        }
                    }
                }

                //--------------------------------------------------------------
                stage('astyle') {
                    agent {
                        docker {
                            reuseNode true
                            alwaysPull true
                            registryUrl DOCKER_BUILD_ENV.registry
                            image DOCKER_BUILD_ENV.image
                            args DOCKER_BUILD_ENV.args
                        }
                    }
                    steps {
                        print_step_info env.STAGE_NAME

                        // Continue with the SDK package build even on style
                        // errors, but mark the stage and the build as failed
                        // already.
                        catchError(buildResult: 'FAILURE', stageResult: 'FAILURE') {
                            run_shell_script('scm-src/astyle_check_sdk.sh')
                        }
                    }
                }

                //--------------------------------------------------------------
                stage('build_hello_world') {
                    // Do a quick sanity test for the SDK package by building
                    // the "hello world" demo. If it works, it makes sense to
                    // start the time consuming builds and tests of all systems.
                    // But if it fails, we can stop here early.
                    agent {
                        docker {
                            reuseNode true
                            alwaysPull true
                            registryUrl DOCKER_BUILD_ENV.registry
                            image DOCKER_BUILD_ENV.image
                            args DOCKER_BUILD_ENV.args
                        }
                    }
                    steps {
                        print_step_info env.STAGE_NAME

                        // Put the "hello world" demo into the folder layout
                        // used by the SDK release package.
                        do_checkout(
                            'ss/demo_hello_world',
                            env.BRANCH_NAME,
                            'sdk-package/pkg/demos/demo_hello_world/src'
                        )

                        run_shell_script(
                            'scm-src/build-sdk.sh',
                            [
                                'build-demos', // mode
                                'sdk-package'  // base dir
                            ]
                        )
                    }
                }

                //--------------------------------------------------------------
                stage('cleanup') {
                    steps {
                        print_step_info env.STAGE_NAME
                        cleanWs()
                    }
                }
            }
        }

        //----------------------------------------------------------------------
        stage('Test SDK') {
            parallel {

                //--------------------------------------------------------------
                stage('publish_docs') {
                    agent { label "publish_doc" }
                    when {
                        beforeAgent true
                        anyOf {
                            branch 'master'
                            branch 'integration'
                        }
                    }
                    steps {
                        print_step_info env.STAGE_NAME
                        cleanWs()
                        unstash('stash_doc')
                        run_shell_script('scm-src/publish_doc.sh', [env.BRANCH_NAME])
                        cleanWs()
                    }
                }

                //--------------------------------------------------------------
                stage('system_tests') {
                    steps {
                        print_step_info env.STAGE_NAME
                        do_jobs_for_test_systems(
                            env.BRANCH_NAME,
                            SYSTEM_CONFIGS)
                    }
                }
            }
        }
    }

    //--------------------------------------------------------------------------
    post {
        always {
            do_notify_bitbucket()
        }
    }
}
