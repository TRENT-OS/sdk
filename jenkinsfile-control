//------------------------------------------------------------------------------
//
// SDK Jenkins Pipeline Control Script
//
// Copyright (C) 2020-2021, HENSOLDT Cyber GmbH
//
//------------------------------------------------------------------------------


def DEV_SDK_PACKAGE = 'dev-sdk-package.tar.bz2'
def SDK_PACKAGE = 'sdk-package.tar.bz2'


//------------------------------------------------------------------------------
def makeParamStr(Object... args)
{
    def paramList = []
    for (arg in args) {
        if (!arg) {
            // ignore null elements
        } else if (arg instanceof String) {
            paramList.add(arg)
        } else if (arg instanceof List) {
            list = arg.flatten()
            list.removeAll([null])
            paramList.addAll(list)
        } else {
            error("Invalid parameter object: " + arg);
            // ignore
        }
    }

    return paramList.join(' ')
}


//------------------------------------------------------------------------------
def buildTarget(String target)
{
    return '-D TEST_CONFIGURATION=' + target
}


//------------------------------------------------------------------------------
def testTarget(String target)
{
    return '--tc=platform.test_configuration:' + target
}


//------------------------------------------------------------------------------
// System context
class SystemCtx {

    static DEMO_LIST = [
        'demo_hello_world',
        'demo_iot_app',
        'demo_iot_app_imx6',
        'demo_iot_app_rpi3',
        'demo_network_filter',
        'demo_tls_api'
    ]

    static FLAG_BUILD      = 1 << 0
    static FLAG_TEST       = 1 << 1
    // For demos included in the SDK package, CI will checkout them from their
    // default repo or use that the SDK package contains at the given location.
    static FLAG_SDK_DEMO   = 1 << 2

    String      systemName   // Name of the system to build.
    Map         platforms    // List of supported platforms.
    Map         params       // Parameters

    public SystemCtx(
        String systemName,
        Map platforms,
        Map params = null)
    {
        this.systemName  = systemName
        this.platforms   = platforms

        // make a copy of the params that can be modified
        this.params = params ? params.clone() : [:]

        this.params.putIfAbsent('flags', 0)

        // SDK demos always use the code from the SDK, thus no custom repo can
        // be specified here. If this happen, clear FLAG_BUILD so there is an
        // error later
        if (0 != (this.params['flags'] & SystemCtx.FLAG_SDK_DEMO)) {
            assert(systemName in DEMO_LIST)
            if (this.params.containsKey('testSystem')) {
                error('custom test system location not supported for SDK demo ' + systemName)
                this.params['flags'] &= ~SystemCtx.FLAG_BUILD
            } else {
                this.params['testSystem'] = 'demos/' + systemName
            }
        }

        def useTest = this.params.getOrDefault('useTest', systemName)
        this.params.putIfAbsent('testSystem', 'ss/' + useTest)
        this.params.putIfAbsent('testScript', useTest + '.py')

    }
}

//------------------------------------------------------------------------------
def BaseFlagCtx(
        String systemName,
        Integer baseFlags,
        Map platforms = null,
        Map params = null
)
{
    def guarded_params = params ? params.clone() : [:]

    if (!guarded_params.putIfAbsent('flags', baseFlags)) {
        guarded_params['flags'] |= baseFlags
    }

    return new SystemCtx(systemName, platforms, guarded_params)
}

//------------------------------------------------------------------------------
def BaseBuildCtx(
        String systemName,
        Map platforms = null,
        Map params = null
)
{
    return BaseFlagCtx(
                systemName,
                SystemCtx.FLAG_BUILD,
                platforms,
                params)
}

//------------------------------------------------------------------------------
def BaseTestCtx(
        String systemName,
        Map platforms = null,
        Map params = null
)
{
    return BaseFlagCtx(
                systemName,
                SystemCtx.FLAG_BUILD | SystemCtx.FLAG_TEST,
                platforms,
                params)
}

//------------------------------------------------------------------------------
// Build/Test configurations to be executed in the generic pipeline.
//
def SYSTEM_CONFIGS = [

    //--------------------------------------------------------------------------
    // Fundamental Demo Systems, required for the SDK package sanity check.
    //--------------------------------------------------------------------------

    BaseBuildCtx(
        'demo_hello_world',
        [
            'zynq7000':    [ addFlags: SystemCtx.FLAG_TEST ],
            'sabre':       [ addFlags: SystemCtx.FLAG_TEST ],
            'spike64':     [ addFlags: SystemCtx.FLAG_TEST ],
            'rpi3':        null,
            'rpi4':        null,
            'nitrogen6sx': null,
            'spike32':     null,
            'zynqmp':      null,
        ],
        [
            addFlags:    SystemCtx.FLAG_SDK_DEMO,
            testScript: 'test_demo_hello_world.py'
        ]
    ),

    //--------------------------------------------------------------------------
    // Additional Official Demo System
    //--------------------------------------------------------------------------

    BaseBuildCtx(
        'demo_iot_app',
        [
            'zynq7000': [ addFlags: SystemCtx.FLAG_TEST ],
            'zynqmp':   null,
        ],
        [
            addFlags:    SystemCtx.FLAG_SDK_DEMO,
            testScript: 'test_demo_iot_app.py'
        ]
    ),

    BaseBuildCtx(
        'demo_iot_app_imx6',
        [
            'sabre':       null,
            'nitrogen6sx': null,
        ],
        [
            addFlags: SystemCtx.FLAG_SDK_DEMO,
        ]

    ),

    BaseBuildCtx(
        'demo_iot_app_rpi3',
        [
            'rpi3': null,
        ],
        [
            addFlags: SystemCtx.FLAG_SDK_DEMO,
        ]

    ),

    BaseBuildCtx(
        'demo_tls_api',
        [
            'zynq7000':    [ addFlags: SystemCtx.FLAG_TEST ],
            'sabre':       [ addFlags: SystemCtx.FLAG_TEST ],
            'nitrogen6sx': null,
            'rpi3':        null
        ],
        [
            addFlags:    SystemCtx.FLAG_SDK_DEMO,
            testScript: 'test_demo_tls_api.py'
        ]

    ),

    BaseBuildCtx(
        'demo_network_filter',
        [
            'zynq7000':    null,
            'nitrogen6sx': null,
        ],
        [
            addFlags: SystemCtx.FLAG_SDK_DEMO,
        ]

    ),

    //--------------------------------------------------------------------------
    // Unofficial Demo Systems
    //--------------------------------------------------------------------------

    BaseBuildCtx(
        'demo_i2c',
        [
            'sabre': null
        ]
    ),

    BaseBuildCtx(
        'demo_tls_server',
        [
            'zynq7000': null
        ]
    ),

    // This test was used during porting the RPi Ethernet driver only. The
    // RasPi3 Socket API of this test needs to be adapted to the recent changes.
    // But this test system is to be deleted anyway, once it's confirmed that
    // all relevant parts were integrated into test_network_api.
    // BaseBuildCtx('demo_raspi_ethernet', ['rpi3': null]),

    //--------------------------------------------------------------------------
    // Test Systems
    //
    // Testing ZynqMP requires using the test container trentos_test:20210316
    // which contains the Xilinx QEMU.
    //--------------------------------------------------------------------------

    BaseBuildCtx(
        'test_timeserver',
        [
            'zynq7000': [ addFlags: SystemCtx.FLAG_TEST ],
            'sabre':    [ addFlags: SystemCtx.FLAG_TEST ],
            'rpi3':     null,
            'rpi4':     null,
            'zynqmp':   null,
        ]
    ),

    BaseBuildCtx(
        'test_certserver',
        [
            'zynq7000': [ addFlags: SystemCtx.FLAG_TEST ],
            'sabre':    [ addFlags: SystemCtx.FLAG_TEST ],
            'rpi3':     null,
            'zynqmp':   null,
        ]
    ),

    BaseBuildCtx(
        'test_uart',
        [
            'zynq7000': [ addFlags: SystemCtx.FLAG_TEST ],
            'sabre':    [ addFlags: SystemCtx.FLAG_TEST ],
            'zynqmp':   null,
        ]
    ),

    BaseBuildCtx(
        'test_chanmux',
        [
            'zynq7000': [ addFlags: SystemCtx.FLAG_TEST ],
            'zynqmp':   null,
        ]
    ),

    BaseBuildCtx(
        'test_proxy_nvm',
        [
            'zynq7000': [ addFlags: SystemCtx.FLAG_TEST ],
            'zynqmp':   null,
        ]
    ),

    BaseBuildCtx(
        'test_certparser',
        [
            'zynq7000': [ addFlags: SystemCtx.FLAG_TEST ],
            'zynqmp':   null,
        ]
    ),

    BaseBuildCtx(
        'test_crypto_api',
        [
            'zynq7000': [ addFlags: SystemCtx.FLAG_TEST ],
            'spike64':  [ addFlags: SystemCtx.FLAG_TEST ],
            'zynqmp':   null,
        ]
    ),

    BaseBuildCtx(
        'test_cryptoserver',
        [
            'zynq7000': [ addFlags: SystemCtx.FLAG_TEST ],
            'zynqmp':   null,
        ]
    ),

    BaseBuildCtx(
        'test_entropysource',
        [
            'zynq7000': [ addFlags: SystemCtx.FLAG_TEST ],
            'zynqmp':   null,
        ]
    ),

    BaseBuildCtx(
        'test_storage_interface',
        [
            'zynq7000': [ addFlags: SystemCtx.FLAG_TEST ],
            'sabre':    [ addFlags: SystemCtx.FLAG_TEST ],
            'rpi3':     null,
            'rpi4':     null,
        ],
        [
            testScript: 'test_storage_interface*.py'
        ]
    ),

    BaseBuildCtx(
        'test_filesystem',
        [
            'zynq7000': [ addFlags: SystemCtx.FLAG_TEST ],
            'zynqmp':   null,
        ]
    ),

    BaseBuildCtx(
        'test_config_server',
        [
            'zynq7000': [ addFlags: SystemCtx.FLAG_TEST ],
            'zynqmp':   null,
        ]
    ),

    BaseBuildCtx(
        'test_keystore',
        [
            'zynq7000': [ addFlags: SystemCtx.FLAG_TEST ],
            'zynqmp':   null,
        ]
    ),

    BaseBuildCtx(
        'test_logserver',
        [
            'zynq7000': [ addFlags: SystemCtx.FLAG_TEST ],
            'zynqmp':   null,
        ]
    ),

    BaseBuildCtx(
        'test_tls_api',
        [
            'zynq7000':    [ addFlags: SystemCtx.FLAG_TEST ],
            'sabre':       null,
            'nitrogen6sx': null,
            'rpi3':        null,
        ]
    ),

    BaseBuildCtx(
        'test_tlsserver',
        [
            'zynq7000':    [ addFlags: SystemCtx.FLAG_TEST ],
            'sabre':       null,
            'nitrogen6sx': null,
            'rpi3':        null,
        ]
    ),

    BaseTestCtx(
        'test_network_api_tcp_client',
        [
            'zynq7000': null,
            'sabre':    null,
        ],
        [
            useTest:    'test_network_api',
            buildParams: buildTarget('tcp_client_single_socket'),
            testParams:  testTarget('tcp_client_single_socket'),
        ]
    ),

    BaseTestCtx(
        'test_network_api_tcp_client_multiple_sockets',
        [
            'zynq7000': null,
            'sabre':    null,
        ],
        [
            useTest:    'test_network_api',
            buildParams: buildTarget('tcp_client_multiple_sockets'),
            testParams:  testTarget('tcp_client_multiple_sockets'),
        ]
    ),

    BaseTestCtx(
        'test_network_api_tcp_client_multiple_clients',
        [
            'zynq7000': null,
            'sabre':    null,
        ],
        [
            useTest:    'test_network_api',
            buildParams: buildTarget('tcp_client_multiple_clients'),
            testParams:  testTarget('tcp_client_multiple_clients'),
        ]
    ),

    BaseBuildCtx(
        'test_network_api_tcp_server',
        [
            'zynq7000':    [ addFlags: SystemCtx.FLAG_TEST ],
            'sabre':       [ addFlags: SystemCtx.FLAG_TEST ],
            'nitrogen6sx': null,
            'rpi3':        null,
        ],
        [
            useTest:    'test_network_api',
            buildParams: buildTarget('tcp_server'),
            testParams:  testTarget('tcp_server'),
        ]
    ),

    BaseTestCtx(
        'test_network_api_udp_server',
        [
            'zynq7000': null,
            'sabre':    null,
        ],
        [
            useTest:    'test_network_api',
            buildParams: buildTarget('udp_server'),
            testParams:  testTarget('udp_server'),
        ]
    )

]


//------------------------------------------------------------------------------
// Docker
//
// Notes:
// * bind the localtime to docker container to avoid problems of gaps between
//   the localtime of the container and the host.
// * add user to group "stack" (1001) in order to grant usage of Haskell stack
//   in the docker image
//
// ToDo
// * why are sudo right needed in the test container
//

def DOCKER_BASE = 'docker:5000'
def DOCKER_REGISTRY = 'https://' + DOCKER_BASE

def DOCKER_BUILD_ENV = [
    registry: DOCKER_REGISTRY,
    image:    DOCKER_BASE + '/trentos_build:20210503',
    args:     ['-v /etc/localtime:/etc/localtime:ro',
               '--group-add=stack',
               '--group-add=sudo'
              ].join(' ')
]

def DOCKER_TEST_ENV  = [
    registry: DOCKER_REGISTRY,
    image:    DOCKER_BASE + '/trentos_test:20211217',
    args:     ['-v /home/jenkins/.ssh/:/home/jenkins/.ssh:ro',
               '-v /etc/localtime:/etc/localtime:ro',
               '--network=bridge',
               '--cap-add=NET_ADMIN',
               '--cap-add=NET_RAW',
               '--device=/dev/net/tun',
               '--group-add=sudo'
              ].join(' ')
]



//------------------------------------------------------------------------------
def print_step_info(name)
{
    println('######## ' + name)
}


//------------------------------------------------------------------------------
def do_notify_bitbucket()
{
    // the StashNotifier requires to run in a node block, which should not be
    // necessary technically. See also the open issue issue at
    // https://github.com/jenkinsci/stashnotifier-plugin/issues/234

    if (env.NODE_NAME)
    {
        notifyBitbucket()
    }
    else
    {
        // If we don't specify any node here explicitly, it will not run on the
        // master node even if it is free, because the master is reserved
        // exclusively for bound jobs. If all other executors are busy, the
        // whole job will stuck just for sending a notification. Thus we use the
        // master node for the notification.
        node('pipeline-control') { notifyBitbucket() }
    }
}


//------------------------------------------------------------------------------
def do_stash(name, file_list)
{
    println('stashing to ' + name)
    stash([
        name: name,
        includes: file_list.join(',')
    ])
}


//------------------------------------------------------------------------------
def do_archiveArtifacts(
    artifacts,
    fingerprint = true
) {
    archiveArtifacts([
        artifacts: artifacts,
        fingerprint: fingerprint
    ])

    manager.createSummary("package.png").appendText(
        '<hr>'+
        'adding Artifact(s): <code><strong>' + artifacts + '</strong></code> '+
        '(see <a href="artifact">artifact/</a>)'
    )
}


//------------------------------------------------------------------------------
def save_current_job_log(
    name
) {
    // get the current job's log up to now. One day we should find a nicer way
    // for this than downloading it

    run_app(
        'wget',
        [
            '-q',
            '--no-check-certificate',
            '-O ' + name,
            BUILD_URL + 'consoleText'
        ]
    )
}


//------------------------------------------------------------------------------
def get_build_relative_url(build)
{
    // getAbsoluteUrl() is deprecated and generally regarded as a bad idea for
    // generating HTML page content. This workaround avoids accessing the raw
    // build, which can't be done by scripts by default

    return '/job/' +
            build.getFullProjectName().replace("/", "/job/") +
            '/' +
            build.getNumber()

}


//------------------------------------------------------------------------------
def get_result_ball_img_url(result)
{
    def states = [
        'SUCCESS':  'green.gif', // there is no png
        'UNSTABLE': 'yellow.png',
        'FAILURE':  'red.png',
        'ABORTED':  'aborted.png'
    ]

    return Jenkins.RESOURCE_PATH +
            '/images/16x16/' +
            states.get(result, 'help.png')
}


//------------------------------------------------------------------------------
def run_app(
    application,
    param_array = null
) {
    def cmdLine = application

    if (param_array)
    {
        cmdLine += ' ' + param_array.join(' ')
    }

    sh(cmdLine)
}


//------------------------------------------------------------------------------
def run_shell_script(
    script,
    param_array = null
) {
    run_app(script, param_array)
}


//------------------------------------------------------------------------------
def do_git_checkout(
    repo,
    branch,
    folder = NULL
) {
    println(
        'checkout ' + repo + '@' + branch +
        ( folder ? (' into ' + folder) : '' ) +
        ' ...'
    )

    try
    {
        def scmVars = checkout([
                    poll: false,
                    scm: [
                        $class: 'GitSCM',
                        branches: [
                            [name: branch]
                        ],
                        doGenerateSubmoduleConfigurations: false,
                        extensions: [
                            [
                                $class: 'RelativeTargetDirectory',
                                relativeTargetDir: (folder ? folder : '.')
                            ],

                            [
                                $class: 'SubmoduleOption',
                                disableSubmodules: false,
                                recursiveSubmodules: true,
                                trackingSubmodules: false,
                                parentCredentials: true,
                                // parallel checkouts speed up things, but too
                                // many threads (from too man parallel jobs)
                                // will eventually overload the GIT server and
                                // we get failures due to timeouts. We've seen
                                // this happening with 8 threads, so try 4 now.
                                threads: 4
                            ]
                        ],
                        userRemoteConfigs: [
                            [
                                credentialsId: scm.userRemoteConfigs[0].credentialsId,
                                url: 'ssh://git@git-server:7999/' + repo + '.git'
                            ]
                        ]
                    ]
                ])

        // clean up some strange folders jenkins creates for an unknown reason
        if (folder) { run_app('rmdir', [folder+'@tmp']) }

        return scmVars
    }
    catch (Exception e)
    {
        println 'Exception: ' + e
        return null
    }
}


//------------------------------------------------------------------------------
def do_checkout(
    repo,
    branch,
    dir = '.'
) {
    def summary = manager.createSummary("document.png")
    summary.appendText('<hr>Repo: <code><strong>' + repo + '</strong></code>')

    def scmVars = do_git_checkout(repo, branch, dir)

    // if this is a custom branch, then fall back to integration
    if ( (!scmVars) && ( !(branch in ['integration', 'master']) ) )
    {
        def fallback_branch = 'integration'
        summary.appendText(
            ' (using <code><strong>' + fallback_branch + '</strong></code>, '+
            'no branch <code><strong>' + branch + '</strong></code>)'
        )
        scmVars = do_git_checkout(repo, fallback_branch, dir)
    }

    if (!scmVars)
    {
        error('checkout error for: ' + branch + '@' + repo)
    }
}


//------------------------------------------------------------------------------
def do_checkout_demos(demo_list, test_configs, folder)
{
    for (demo in demo_list)
    {
        do_checkout(
            'ss/' + demo,
            env.BRANCH_NAME,
            folder + '/' + demo)

        // Check if the SDK demo is also built and/or tested.
        if (!test_configs.any {it.systemName == demo})
        {
            error("SDK demo " + demo + " must be included at least once in SYSTEM_CONFIGS.")
        }
    }
}


//------------------------------------------------------------------------------
def execute_build(ctx, stage_ctx)
{
    def flags = stage_ctx.getOrDefault('flags', 0)

    def jobBuild = build(
        job: 'generic_jobs/generic_pipeline_sandbox',
        wait: true,   // block call until finished
        propagate: false, // don't throw exception on error
        parameters: [
            string(
                name: 'PLATFORM',
                value: stage_ctx.getOrDefault('platform', '')
            ),
            string(
                name:  'BRANCH_OR_COMMIT',
                value: ctx.branch ?: ''
            ),
            string(
                name:  'FLAGS',
                value: flags as String // must be made a string
            ),
            string(
                name:  'TEST_SYSTEM',
                value: stage_ctx.getOrDefault('testSystem', '')
            ),
            string(
                name:  'BUILD_PARAMS',
                value: stage_ctx.getOrDefault('buildParams', '')
            ),
            string(
                name:  'TEST_SCRIPT',
                value: stage_ctx.getOrDefault('testScript', '')
            ),
            string(
                name:  'TEST_PARAMS',
                value: stage_ctx.getOrDefault('testParams', '')
            ),
            string(
                name:  'UPSTREAM_JOB_NAME',
                value: currentBuild.fullProjectName
            ),
            string(
                name:  'UPSTREAM_JOB_ID',
                value: currentBuild.id
            ),
        ]
    )

    // if we arrive here, the job has finished
    stage_ctx.build = jobBuild
    def result = jobBuild.getResult()
    def duration = jobBuild.getDurationString()

    lock('job-post-processing')
    {
        println((ctx.cnt - (--ctx.running)) + '/' + ctx.cnt + ': ' +
                stage_ctx.test_ctx.systemName + ' system build/test stage took '
                + duration + ', result: ' + result)

        def operations = []
        if (flags & SystemCtx.FLAG_BUILD) { operations += ['build'] }
        if (flags & SystemCtx.FLAG_TEST) { operations += ['test'] }
        if (flags & SystemCtx.FLAG_SDK_DEMO) { operations += ['demo'] }

        ctx.summary.appendText(
            stage_ctx.platform + ', ' + stage_ctx.test_ctx.systemName + ':' +
            ' <a href="' + get_build_relative_url(jobBuild) + '">' +
                '#' + jobBuild.getNumber() +
            '</a>' +
            ' <img src="' + get_result_ball_img_url(result) + '">' +
            (operations ? (' ' + operations.join(', ')) : '') +
            ' (' + duration + ')' +
            '<br>',
            false)
    }

    // When CI is under heavy load, no free executor could be available to run
    // the artifact copy from the child job. In the worst case the child job
    // already got deleted. We could extend the time before the job is deleted,
    // but this may overload CI also because the workspaces eat the disk space.
    // Better solution is running the copy job on a dedicated executor, which
    // exists on the master
    node('pipeline-control')
    {
        cleanWs()

        catchError(buildResult: 'UNSTABLE', stageResult: 'FAILURE')
        {
            copyArtifacts(
                projectName: jobBuild.getFullProjectName(),
                // need to ensure this is a string
                selector: specific("${jobBuild.getNumber()}")
            )

            do_stash(
                stage_ctx.name,
                [
                    'system-package.tar.bz2',
                    'test_results.xml'
                ]
            )
        }

        cleanWs()
    }

    // We have set propagate=false above, so we have to deal with the failures
    // here. Since all this runs in block wrapped by catchError(), we can't
    // really fail the build here, but we can ensure a message is logged and
    // the state is set properly.
    if ('SUCCESS' != result)
    {
        def msg = 'test ' + result + ' for ' + stage_ctx.platform + ', ' +
                  stage_ctx.test_ctx.systemName

        if ('UNSTABLE' == result) {
            // propagate unstable
            unstable(msg)
        }
        else {
            // 'FAILURE', 'ABORTED' is fatal
            error(msg)
        }
    }
}


//------------------------------------------------------------------------------
def do_jobs_for_test_systems(branch, testConfigs)
{
    // all downstream job packages are packed into this single package
    def SYSTEM_PACKAGES = 'system-packages.tar.bz2'

    def summary = manager.createSummary("orange-square.png")
    summary.appendText('<hr>')

    def jobStages = []
    for (test_ctx in testConfigs) {

        def baseFlags = test_ctx.params?.get('flags') ?: 0
        def testSystem = test_ctx.params?.get('testSystem')

        if (!testSystem) {
            // Test system is initialized to 'null' in the constructor if a
            // non-default repo is specified for an SDK demo, which is not
            // supported.
            error("SDK demo must use default repo: " + test_ctx.systemName)
            continue // this is not a blocker for other tests
        }

        for (plat in test_ctx.platforms) {

            def platName = plat.key
            def platParams = plat.value
            def flags = baseFlags | (platParams?.get('addFlags') ?: 0)
            def isBuild = (flags & SystemCtx.FLAG_BUILD)
            def isTest = (flags & SystemCtx.FLAG_TEST)

            // Currently, a test always requires a build. But in the future,
            // testing an existing system might also be supported.
            if (!isBuild) {
                error("no built requested for " + test_ctx.systemName)
                continue
            }

            jobStages.add([
                name:        platName + '-' + test_ctx.systemName,
                test_ctx:    test_ctx,
                platform:    platName,
                flags:       flags,
                testSystem:  testSystem,

                // put platform params after test_ctx params
                buildParams: makeParamStr(
                                test_ctx.params?.get('buildParams'),
                                platParams?.get('buildParams')),

                // platform setting takes preference over test_ctx setting
                testScript:  !isTest ? null :
                             platParams?.get('testScript') ?:
                             test_ctx.params?.get('testScript') ?:
                             null,

                // put platform params after test_ctx params
                testParams:  !isTest ? null :
                             makeParamStr(
                                test_ctx.params?.get('testParams'),
                                platParams?.get('testParams')),

                build:       null // will be set once there is a build
            ])
        }
    }

    def sub_jobs_ctx = [
        summary:   summary,
        branch:    branch,
        cnt:       jobStages.size(),
        running:   jobStages.size(),
    ]

    def build_start = new Date()

    catchError(buildResult: 'UNSTABLE', stageResult: 'FAILURE')
    {
        // we don't need an executor to start the jobs and wait for them to
        // finish
        //
        //  parallel
        //      firstBranch:  { /* do something */ },
        //      secondBranch: { /* do something */ },
        //      failFast: true|false // terminate all upon any one failing

        parallel(
            // Create a map, where each key is an arbitrary name and the value
            // is a closure with the code that will be executed in parallel
            // (potentially distributed) jobs.
            jobStages.collectEntries { stage_ctx ->
                [
                    ( stage_ctx.name ):
                    { ->
                        stage(stage_ctx.name) {
                            execute_build(sub_jobs_ctx, stage_ctx)
                        }
                    }
                ]
            }
        )
    }

    duration = groovy.time.TimeCategory.minus(new Date(), build_start)
    def duration_msg = 'all system build/test stages took ' + duration
    println(duration_msg)
    summary.appendText('<hr>' + duration_msg, false)

    // builds are done, now collect the results and artifacts. This needs a
    // workspace, so we have to allocate a node

    def was_build_error = false

    node {
        stage('process_results') {

            cleanWs()

            jobStages.eachWithIndex { stage_ctx, idx ->

                println('processing ' + (idx+1) + '/' + jobStages.size() + ': ' +
                        stage_ctx.name)

                // there might be no stash if something failed earlier
                catchError(buildResult: 'UNSTABLE', stageResult: 'FAILURE')
                {
                    def pkg_dir = 'package-' + stage_ctx.name
                    dir(pkg_dir)
                    {
                        unstash(stage_ctx.name)
                    }
                }

                def pkg_build = stage_ctx.build
                was_build_error |= (!pkg_build) ||
                                   (pkg_build.getResult() in ['FAILURE','ABORTED'])
            }

            // Sometimes this job runs just to test the SDK generation, so
            // there are no test systems enabled. We don't want to see things
            // fail with junit complaining about missing test files in this
            // case.
            if (jobStages.size() > 0)
            {
                // TODO: Support multiple platforms here, currently this will
                //       see the same test case for each platform and it's
                //       likely messing up the analysis.
                junit('package-*/test_results.xml')
            }

            save_current_job_log('build.log')

            // Create a package of all files, so the next stage can process
            // them. Ensure we never fail here if a file is missing, because
            // there is not much we can do anyway. The next stage is supposed
            // to handle this.
            run_app(
                'tar',
                [
                    '--ignore-failed-read',
                    '-cjf ' + SYSTEM_PACKAGES,
                    'build.log',
                    'package-*/'
                ]
            )

            do_archiveArtifacts(SYSTEM_PACKAGES)

            cleanWs()

        }
    }

    if (was_build_error)
    {
        error('stage failed: run test systems')
    }
}


//------------------------------------------------------------------------------
pipeline {

    agent none

    options {

        skipDefaultCheckout()

        // disableConcurrentBuilds()

        // unfortunately there is no way to have build discarding conditional
        // for development branches only, so "master" and "integration" would
        // keep their builds. The potential work around is having a conditional
        // stage where "master" and "integration" push a package to an artifact
        // server, which then preserves this.
        buildDiscarder(logRotator(numToKeepStr: '20'))
    }

    stages {

        //----------------------------------------------------------------------
        stage('Build SDK') {

            agent { label "build" }

            stages {

                //--------------------------------------------------------------
                stage('checkout') {
                    steps {
                        print_step_info env.STAGE_NAME
                        cleanWs()
                        checkout([
                            $class: 'GitSCM',
                            branches: scm.branches,
                            doGenerateSubmoduleConfigurations: false,
                            extensions: [
                                [
                                    $class: 'RelativeTargetDirectory',
                                    relativeTargetDir: 'scm-src'
                                ],
                                [
                                    $class: 'SubmoduleOption',
                                    disableSubmodules: false,
                                    recursiveSubmodules: true,
                                    trackingSubmodules: false,
                                    parentCredentials: true,
                                    threads: 4,
                                ]
                            ],
                            userRemoteConfigs: scm.userRemoteConfigs
                        ])

                        // WARNING: The CI job has to checkout the demos into a
                        //          folder structure like the one of seos_tests.
                        do_checkout_demos(SystemCtx.DEMO_LIST, SYSTEM_CONFIGS, 'src/demos')

                        do_notify_bitbucket()
                    }
                }

                //--------------------------------------------------------------
                stage('package') {
                    agent {
                        docker {
                            reuseNode true
                            alwaysPull true
                            registryUrl DOCKER_BUILD_ENV.registry
                            image DOCKER_BUILD_ENV.image
                            args DOCKER_BUILD_ENV.args
                        }
                    }
                    steps {
                        print_step_info env.STAGE_NAME
                        run_shell_script(
                            'scm-src/build-sdk.sh',
                            [
                                'package',    // mode
                                'sdk-package' // SDK package output base dir
                            ]
                        )
                    }
                }

                //--------------------------------------------------------------
                stage('archive') {
                    steps {
                        print_step_info env.STAGE_NAME

                        do_archiveArtifacts(DEV_SDK_PACKAGE)
                        do_archiveArtifacts(SDK_PACKAGE)
                        do_archiveArtifacts('sdk-package/version.info')

                        do_stash(
                            'stash_sdk_packages',
                            [
                                'scm-src/build-sdk.sh',
                                DEV_SDK_PACKAGE,
                                SDK_PACKAGE
                            ]
                        )

                        do_stash(
                            'stash_doc',
                            [
                                'scm-src/publish_doc.sh',
                                'sdk-package/pkg/doc/'
                            ]
                        )
                    }
                }

                //--------------------------------------------------------------
                stage('publish_docs') {
                    agent { label "publish_doc" }
                    when {
                        beforeAgent true
                        anyOf {
                            branch 'master'
                            branch 'integration'
                        }
                    }
                    steps {
                        print_step_info env.STAGE_NAME
                        cleanWs()
                        unstash('stash_doc')
                        run_shell_script('scm-src/publish_doc.sh', [env.BRANCH_NAME])
                    }
                }

                //--------------------------------------------------------------
                stage('cleanup') {
                    steps {
                        print_step_info env.STAGE_NAME
                        cleanWs()
                    }
                }
            }
        }

        //----------------------------------------------------------------------
        stage('Test SDK') {
            parallel {

                //--------------------------------------------------------------
                stage('unit_test') {
                    agent {
                        docker {
                            reuseNode true
                            alwaysPull true
                            registryUrl DOCKER_TEST_ENV.registry
                            image DOCKER_TEST_ENV.image
                            args DOCKER_TEST_ENV.args
                        }
                    }
                    steps {
                        print_step_info env.STAGE_NAME

                        // Ensure workspace is clean and unstash required files.
                        run_app('rm', ['*', '-rf'])
                        unstash('stash_sdk_packages')

                        // Extract the DEV_SDK_PACKAGE. The subfolder 'pkg' is
                        // required by the SDK build script.
                        run_app('mkdir', ['-p', 'unit_test_sdk/pkg'])
                        run_app('tar', ['-xf ' + DEV_SDK_PACKAGE, '-C unit_test_sdk/pkg'])

                        catchError(buildResult: 'UNSTABLE', stageResult: 'FAILURE') {
                            run_shell_script(
                                'scm-src/build-sdk.sh',
                                [
                                    'run-unit-tests', // mode
                                    'unit_test_sdk' // base dir
                                ]
                            )
                        }
                    }
                }

                //--------------------------------------------------------------
                stage('astyle') {
                    agent {
                        docker {
                            reuseNode true
                            alwaysPull true
                            registryUrl DOCKER_BUILD_ENV.registry
                            image DOCKER_BUILD_ENV.image
                            args DOCKER_BUILD_ENV.args
                        }
                    }
                    steps {
                        print_step_info env.STAGE_NAME

                        // Ensure workspace is clean and unstash required files.
                        run_app('rm', ['*', '-rf'])
                        unstash('stash_sdk_packages')

                        // Extract the DEV_SDK_PACKAGE. The subfolder 'pkg' is
                        // required by the SDK build script.
                        run_app('mkdir', ['-p', 'astyle_sdk/pkg'])
                        run_app('tar', ['-xf ' + DEV_SDK_PACKAGE, '-C astyle_sdk/pkg'])

                        // Continue with the SDK package build even on style
                        // errors, but mark the stage and the build as failed
                        // already.
                        catchError(buildResult: 'FAILURE', stageResult: 'FAILURE') {
                            run_shell_script('astyle_sdk/pkg/astyle_check_sdk.sh')
                        }
                    }
                }

                //--------------------------------------------------------------
                stage('sanity_check') {
                    // Do a quick sanity test for the SDK package by building
                    // the demo Hello World for zynq7000. Only if it works, the
                    // time consuming builds and tests of all systems are done.
                    agent {
                        docker {
                            reuseNode true
                            alwaysPull true
                            registryUrl DOCKER_BUILD_ENV.registry
                            image DOCKER_BUILD_ENV.image
                            args DOCKER_BUILD_ENV.args
                        }
                    }
                    steps {
                        print_step_info env.STAGE_NAME

                        // Ensure workspace is clean and unstash required files.
                        run_app('rm', ['*', '-rf'])
                        unstash('stash_sdk_packages')

                        // Extract the SDK_PACKAGE. The subfolder 'pkg' is
                        // required by the SDK build script.
                        run_app('mkdir', ['-p', 'sanity_check_sdk/pkg'])
                        run_app('tar', ['-xf ' + SDK_PACKAGE, '-C sanity_check_sdk/pkg'])

                        // Build demo Hello World.
                        run_shell_script(
                            'scm-src/build-sdk.sh',
                            [
                                'sanity-check', // mode
                                'sanity_check_sdk' // base dir
                            ]
                        )
                    }
                }
            }
        }

        //----------------------------------------------------------------------
        stage('Test Systems') {
            steps {
                print_step_info env.STAGE_NAME
                do_jobs_for_test_systems(
                    env.BRANCH_NAME,
                    SYSTEM_CONFIGS)
            }
        }
    }

    //--------------------------------------------------------------------------
    post {
        always {
            do_notify_bitbucket()
        }
    }
}
