//------------------------------------------------------------------------------
//
// Generic Jenkins Pipeline Script
//
// Copyright (C) 2020-2021, HENSOLDT Cyber GmbH
//
//------------------------------------------------------------------------------

// Global variables defined with 'def' cannot be accessed in methods due to
// different scoping. Without 'def', they can be accessed - however, that is bad
// practice. Using 'def' and the '@groovy.transform.Field' annotation is the
// best option. For details see https://stackoverflow.com/questions/50571316

//------------------------------------------------------------------------------
// Build Parameters

@groovy.transform.Field
def PLATFORM = null

@groovy.transform.Field
def SDK_SOURCE = null

@groovy.transform.Field
def TEST_SYSTEM = null

@groovy.transform.Field
def BUILD_PARAMS = null

@groovy.transform.Field
def TEST_SCRIPT = null

@groovy.transform.Field
def TEST_PARAMS = null

@groovy.transform.Field
def DOCKER_BUILD_ENV = null

@groovy.transform.Field
def DOCKER_TEST_ENV = null

@groovy.transform.Field
def SYSTEM_DIR = null  // internal parameter only

@groovy.transform.Field
def IS_DEMO = false  // internal parameter only

@groovy.transform.Field
def TEST_NODE = null  // internal parameter only


//------------------------------------------------------------------------------
// Constants

@groovy.transform.Field
def final NODE_LABEL_CONTROL = 'pipeline-control'

@groovy.transform.Field
def final NODE_LABEL_BUILD = 'build'

@groovy.transform.Field
def final NODE_LABEL_TEST = 'test'

@groovy.transform.Field
def final SDK_PACKAGE = 'sdk-package.tar.bz2'

@groovy.transform.Field
def final DEV_SDK_PACKAGE = 'dev-sdk-package.tar.bz2'

@groovy.transform.Field
def final SYSTEM_PACKAGE = 'system-package.tar.bz2'

@groovy.transform.Field
def final TEST_RESULT_PACKAGE = 'test-result-package.tar.bz2'

@groovy.transform.Field
def final SDK_DIR = 'src/sdk'

@groovy.transform.Field
def final BUILD_DIR = 'build'

@groovy.transform.Field
def final TESTS_DIR = 'src/tests'

@groovy.transform.Field
def final TEST_COMMON_DIR = 'src/test-common'

@groovy.transform.Field
def final STASH_TEST_PACKAGE = 'test_package'

@groovy.transform.Field
def final CI_HW_BOARDS = [
    // 'sabre',
    // 'nitrogen6sx',
    // 'rpi3',
    // 'rpi4',
]


//------------------------------------------------------------------------------
def do_pipeline_init() {

    // Jenkins provides a global variable "params" with all build parameters.
    // BRANCH_OR_COMMIT must be dedicated a parameter, because the job's config
    // uses this to determine where to get the jenkinsfile from. All other
    // parameters are passed via PARAMS_JSON as JSON. This allows passing
    // arbitrary parameters without having to make the jenkins job aware of any
    // details.

    // params.each { println("${it.key} = ${it.value}") }

    assert(BRANCH_OR_COMMIT)

    def json_params = readJSON([
        text: params.get('PARAMS_JSON'),
        returnPojo: true])

    // json_params.each { println("JSON: ${it.key} = ${it.value}") }

    flags = json_params.getOrDefault('FLAGS', [])
    assert(flags instanceof List)

    PLATFORM = json_params.get('PLATFORM')
    assert(PLATFORM instanceof String)

    SDK_SOURCE = json_params.get('SDK_SOURCE')
    assert(SDK_SOURCE instanceof String)

    test_system_url = json_params.get('TEST_SYSTEM')
    assert(test_system_url instanceof String)
    def m = (test_system_url =~ '(.*?)://(.*)')
    assert(m.matches())
    switch(m.group(1)) {
        case 'pkg':
            // a relative path within SDK_DIR
            TEST_SYSTEM = m.group(2)
            SYSTEM_DIR = "${SDK_DIR}/${TEST_SYSTEM}"
            break
        case 'repo':
            // a repo location to checkout from
            TEST_SYSTEM = m.group(2)
            SYSTEM_DIR = 'src/system'
            break
        default:
            error("invalid parameter TEST_SYSTEM: ${TEST_SYSTEM}")
    }

    DOCKER_BUILD_ENV = json_params.get('DOCKER_BUILD_ENV')
    assert(DOCKER_BUILD_ENV instanceof Map)

    DOCKER_TEST_ENV = json_params.get('DOCKER_TEST_ENV')
    assert(DOCKER_TEST_ENV instanceof Map)

    // these params are just empty if not set explicitly
    TEST_SCRIPT = json_params.getOrDefault('TEST_SCRIPT','')
    BUILD_PARAMS = json_params.getOrDefault('BUILD_PARAMS','')
    TEST_PARAMS = json_params.getOrDefault('TEST_PARAMS','')

    IS_DEMO = 'SDK_DEMO' in flags

    if ('NO_TEST' in flags) {
        // The test is explicitly disabled.
        assert TEST_NODE == null
    } else if (!TEST_SCRIPT) {
        // Running a test requires a test script.
        error("missing TEST_SCRIPT")
    } else if ('QEMU' in flags) {
        // QEMU tests can run on generic test nodes
        TEST_NODE = NODE_LABEL_TEST
    } else if (PLATFORM in CI_HW_BOARDS) {
        TEST_NODE = PLATFORM
    } else {
        // We can't run a test if it's not running in QEMU and there is no
        // board. Don't raise an error here, as we can still do a build. Just
        // the test will fail then. Since there are no boards in CI so far,
        // just consider all board tests as implicitly disabled.
        assert(CI_HW_BOARDS.size() == 0)
        assert(TEST_NODE == null)
    }

    def info = "${BRANCH_OR_COMMIT}, ${TEST_SYSTEM}, ${PLATFORM}"
    def ops = []
    if (IS_DEMO) { ops.add('demo') }
    if (TEST_NODE) { ops.add('test') }
    if ('QEMU' in flags) { ops.add('qemu') }
    if (ops) { info += " [${ops.join(', ')}]" }
    currentBuild.displayName = info

    manager.createSummary("text.gif").appendText(
        '<table style="border: 1px solid black; border-collapse: collapse;">'+
            params.collect { param ->
                '<tr>'+
                    '<td style="border: 1px solid black">'+
                        '<b><code>'+param.key+'</code></b>'+
                    '</td>'+
                    '<td style="border: 1px solid black">'+
                        (param.value?:'<i>(null)</i>')+
                    '</td>'+
                '</tr>'
            }.join('') +
        '</table>'
    )
}


//------------------------------------------------------------------------------
def do_notify_bitbucket()
{
    // the StashNotifier requires to run in a node block, which should not be
    // necessary technically. See also the open issue issue at
    // https://github.com/jenkinsci/stashnotifier-plugin/issues/234

    if (env.NODE_NAME)
    {
        notifyBitbucket()
    }
    else
    {
        // If we don't specify any node here explicitly, it will not run on the
        // master node even if it is free, because the master is reserved
        // exclusively for bound jobs. If all other executors are busy, the
        // whole job will stuck just for sending a notification. Thus we use the
        // master node for the notification.
        node(label: NODE_LABEL_CONTROL) { notifyBitbucket() }
    }
}


//------------------------------------------------------------------------------
def run_app(
    application,
    param_array = null
) {
    def cmdLine = application

    if (param_array)
    {
        cmdLine += " ${param_array.join(' ')}"
    }

    try
    {
        sh(cmdLine)
    }
    catch (Exception e)
    {
        println("Exception running command: ${e}")
        throw e
    }
}


//------------------------------------------------------------------------------
def run_shell_script(
    script,
    param_array = null
) {
    run_app(script, param_array)
}


//------------------------------------------------------------------------------
def archive_as_tar_bz2(name, files = null)
{
    def params = [
        '-c',
        '-j',
        "-f ${name}",
        '--ignore-failed-read',
        '--sort=name',
        '--numeric-owner',
        '--owner=0',
        '--group=0',
    ]

    if (files) {
        params += files
    }

    run_app('tar', params)
    archiveArtifacts(artifacts: name, fingerprint: true)
}


//------------------------------------------------------------------------------
def run_python_package(
    python_params,
    python_paths,
    python_package, // can't use 'package', it's a reserved groovy keyword
    package_params = []
) {
    def env = []
    def params = []

    if (python_paths)
    {
       env += ["PYTHONPATH=${python_paths.join(':')}"]
    }

    if (python_params)
    {
        params += python_params
    }

    params += ['-m', python_package]

    if (package_params)
    {
        params += package_params
    }

    withEnv(env)
    {
        run_app('python3', params)
    }
}


//------------------------------------------------------------------------------
def do_git_checkout(
    repo,
    branch,
    folder = NULL
) {
    println("checkout ${repo}@${branch}" +
            (folder ? " into ${folder}" : '') + ' ...')

    try
    {
        def scm_cfg = scm.userRemoteConfigs[0]

        // use project URL to build repo URL
        def m = (scm_cfg.url =~ '(.*?)://([^/]*)(?:/(.*))?')
        if (!m.matches()) {
            throw new Exception("could not parse url: ${url}")
        }
        def repo_url = "${m.group(1)}://${m.group(2)}/${repo}.git"

        def scmVars = checkout([
                    poll: false,
                    changelog: false,
                    scm: [
                        $class: 'GitSCM',
                        branches: [
                            [name: branch]
                        ],
                        doGenerateSubmoduleConfigurations: false,
                        extensions: [
                            [
                                $class: 'RelativeTargetDirectory',
                                relativeTargetDir: (folder ? folder : '.')
                            ],

                            [
                                $class: 'SubmoduleOption',
                                disableSubmodules: false,
                                recursiveSubmodules: true,
                                trackingSubmodules: false,
                                parentCredentials: true,
                                // parallel checkouts speed up things, but too
                                // many threads (from too man parallel jobs)
                                // will eventually overload the GIT server and
                                // we get failures due to timeouts. We've seen
                                // this happening with 8 threads, so try 4 now.
                                threads: 4
                            ]
                        ],
                        userRemoteConfigs: [
                            [
                                credentialsId: scm_cfg.credentialsId,
                                url: repo_url
                            ]
                        ]
                    ]
                ])

        // delete additional folder that is created by the checkout
        if (folder) { run_app('rmdir', ["${folder}@tmp"]) }

        return scmVars
    }
    catch (Exception e)
    {
        println("Exception: ${e}")
        return null
    }
}


//------------------------------------------------------------------------------
def do_checkout(
    repo,
    branch,
    dir = '.'
) {
    // if there is no branch with the given name, the fallback is 'integration'
    // and then 'master'.
    def branches = [branch]
    if ('master' != branch) {
        if ('integration' != branch) {
            branches.add('integration')
        }
        branches.add('master')
    }

    for (try_branch in branches)
    {
        if (do_git_checkout(repo, try_branch, dir))
        {
            return
        }
    }

    error("checkout error for ${repo} no branches: ${branches.join(', ')}")
}


//------------------------------------------------------------------------------
def get_sdk(
    package_name
) {
    def m = (SDK_SOURCE =~ 'job://(.*):(.*)')
    if (!m.matches()) {
        throw new Exception("unsupported SDK source: ${SDK_SOURCE}")
        // ToDo: if the parameter is left empty, we could add a convenience
        //       feature that currentBuild.getBuildCauses() is checked. If it is
        //       a BuildUpstreamCause then the fields 'upstreamProject' and
        //       'upstreamBuild' could be used.
    }

    def job_name = m.group(1)
    def job_id = Integer.parseInt(m.group(2))

    // This only works in production mode if the job did also call
    // copyArtifactPermission() to give us the permission to copy artifacts.
    copyArtifacts(
        projectName: job_name,
        selector: specific("${job_id}"), // need to ensure this is a string
        filter: package_name,
    )

    run_app('mkdir', ['-p', SDK_DIR])
    run_app('tar', ["-xf ${package_name}", "-C ${SDK_DIR}"])
}


//------------------------------------------------------------------------------
def print_step_info(
    name
) {
    println("#################### ${name}")
}


//------------------------------------------------------------------------------
//------------------------------------------------------------------------------
//------------------------------------------------------------------------------
pipeline {

    agent none

    options {
        skipDefaultCheckout()
    }

    stages {

        //----------------------------------------------------------------------
        stage('Init') {
            steps {
                do_pipeline_init()
            }
        }

        //----------------------------------------------------------------------
        stage('Build Stage') {

            agent { label NODE_LABEL_BUILD }

            stages {

                //--------------------------------------------------------------
                stage('checkout') {
                    steps {
                        print_step_info(env.STAGE_NAME)
                        cleanWs()
                        script {
                            if (IS_DEMO) {
                                // The SDK demos must work out-of-the-box
                                get_sdk(SDK_PACKAGE)
                            } else {
                                // systems use the dev SDK package, it contains
                                // experimental things that don't get released.
                                get_sdk(DEV_SDK_PACKAGE)
                                do_checkout(TEST_SYSTEM, BRANCH_OR_COMMIT, SYSTEM_DIR)
                            }
                        }
                        do_notify_bitbucket()
                    }
                }

                //--------------------------------------------------------------
                stage('build') {
                    agent {
                        docker {
                            reuseNode true
                            alwaysPull true
                            registryUrl DOCKER_BUILD_ENV.registryUrl
                            registryCredentialsId DOCKER_BUILD_ENV.registryCredentialsId
                            image DOCKER_BUILD_ENV.image
                            args DOCKER_BUILD_ENV.args
                        }
                    }
                    steps {
                        print_step_info(env.STAGE_NAME)
                        tee('log-build.txt')
                        {
                            run_shell_script(
                                SDK_DIR+'/build-system.sh',
                                [
                                    SYSTEM_DIR,
                                    PLATFORM,
                                    BUILD_DIR,
                                    '-D CMAKE_BUILD_TYPE=Debug',
                                    BUILD_PARAMS
                                ]
                            )
                        }
                    }
                }

                //--------------------------------------------------------------
                stage('static_analyzer') {
                    agent {
                        docker {
                            reuseNode true
                            alwaysPull true
                            registryUrl DOCKER_BUILD_ENV.registryUrl
                            registryCredentialsId DOCKER_BUILD_ENV.registryCredentialsId
                            image DOCKER_BUILD_ENV.image
                            args DOCKER_BUILD_ENV.args
                        }
                    }
                    steps {
                        print_step_info(env.STAGE_NAME)
                        do_checkout('ta/seos_tests', BRANCH_OR_COMMIT, TESTS_DIR)
                        tee('log-analyzer.txt')
                        {
                            // Continue with the test run even on static analyzer
                            // errors, but mark the stage and the build as failed
                            // even if the tests are successful.
                            catchError(buildResult: 'FAILURE', stageResult: 'FAILURE')
                            {
                                run_shell_script(
                                    TESTS_DIR+'/cppcheck/check_static_analyzer.sh',
                                    [
                                        BUILD_DIR+'/compile_commands.json',
                                        SDK_DIR
                                    ]
                                )
                            }
                        }
                    }
                }

                //--------------------------------------------------------------
                stage('archive_build') {
                    steps {
                        print_step_info(env.STAGE_NAME)

                        archive_as_tar_bz2(
                            SYSTEM_PACKAGE,
                            // Keep the build artifacts minimal and don't
                            // use BUILD_DIR+'/' to get everything
                            [
                                'images/os_image.elf',
                                'graph.svg',
                                'compile_commands.json'
                            ].collect { "${BUILD_DIR}/${it}" } +
                            [
                                'log-build.txt',
                                'log-analyzer.txt'
                            ]
                        )
                    }
                }

                //--------------------------------------------------------------
                stage('cleanup') {
                    steps {
                        print_step_info(env.STAGE_NAME)
                        cleanWs()
                    }
                }

            }
        }

        //----------------------------------------------------------------------
        stage('Test Stage') {
            when {
                beforeAgent true
                expression {
                    if (!TEST_NODE) {
                        return false; // test are not enabled
                    }
                    if (0 == nodesByLabel(TEST_NODE).size()) {
                        unstable("no test node online with label '${TEST_NODE}'")
                        return false
                    }
                    return true
                }
            }
            agent { label TEST_NODE }
            stages {

                //--------------------------------------------------------------
                stage('checkout') {
                    steps {
                        print_step_info(env.STAGE_NAME)
                        cleanWs()
                        copyArtifacts(
                            projectName: JOB_NAME,
                            selector: specific("${BUILD_NUMBER}"), // must be a string
                            filter: SYSTEM_PACKAGE
                        )
                        run_app('tar', ["-xf ${SYSTEM_PACKAGE}"])
                        // The release SDK package lack some files needed for
                        // our tests, thus there is a special dev package.
                        get_sdk(DEV_SDK_PACKAGE)
                        do_checkout('ta/common', BRANCH_OR_COMMIT, TEST_COMMON_DIR)
                        do_checkout('ta/seos_tests', BRANCH_OR_COMMIT, TESTS_DIR)
                    }
                }

                //--------------------------------------------------------------
                stage('test_doc') {
                    agent {
                        docker {
                            reuseNode true
                            alwaysPull true
                            registryUrl DOCKER_TEST_ENV.registryUrl
                            registryCredentialsId DOCKER_TEST_ENV.registryCredentialsId
                            image DOCKER_TEST_ENV.image
                            args DOCKER_TEST_ENV.args
                        }
                    }
                    steps {
                        tee('log-test-doc.txt')
                        {
                            dir('test-doc')
                            {
                                catchError(buildResult: 'UNSTABLE', stageResult: 'FAILURE')
                                {
                                    run_python_package(
                                        ['-B'],
                                        ["../${TEST_COMMON_DIR}", "../${TESTS_DIR}"],
                                        'pydoc',
                                        ['-w', "../${TESTS_DIR}/${TEST_SCRIPT}"]
                                    )
                                }
                            }
                        }
                    }
                }

                //--------------------------------------------------------------
                stage('test') {
                    options {
                        // Limit the time the test can run. This also catches
                        // the cases where the (buggy) test framework doesn't
                        // kill all threads and these threads keep it from
                        // terminating. The longest test execution time seen
                        // so far was 310 seconds for test_uart, so double of
                        // that looks like a good value to use for now.
                        timeout(time: 620, unit: 'SECONDS')
                    }

                    agent {
                        docker {
                            reuseNode true
                            alwaysPull true
                            registryUrl DOCKER_TEST_ENV.registryUrl
                            registryCredentialsId DOCKER_TEST_ENV.registryCredentialsId
                            image DOCKER_TEST_ENV.image
                            args DOCKER_TEST_ENV.args
                        }
                    }

                    steps {
                        print_step_info(env.STAGE_NAME)
                        tee('log-test-run.txt')
                        {
                            dir('test-logs')
                            {
                                catchError(buildResult: 'UNSTABLE', stageResult: 'FAILURE')
                                {
                                    script
                                    {
                                        def prepareTestSystem = "../${SYSTEM_DIR}/prepare_test.sh"
                                        if (fileExists(prepareTestSystem))
                                        {
                                            run_shell_script(
                                                prepareTestSystem,
                                                ['../'+SDK_DIR])
                                        }
                                    }
                                    run_python_package(
                                        ['-B'],
                                        ["../${TEST_COMMON_DIR}", "../${TESTS_DIR}"],
                                        'pytest',
                                        [
                                            // pytest options
                                            '-p', 'no:cacheprovider',
                                            '-v', // verbose output
                                            '-o log_cli=True', // write logs to console
                                            '--capture=no', // show printf() from python scripts in console
                                            '--tb=short', // control traceback (long, short, line, native, no)
                                            // test framework options
                                            '--print_logs', // print system logs in console
                                            "--proxy=../${SDK_DIR}/bin/proxy_app,TCP",
                                            "--target=${PLATFORM}",
                                            "--system_image=../${BUILD_DIR}/images/os_image.elf",
                                            "--resource_dir=../${SDK_DIR}/resources",
                                            '--junitxml=../test_results.xml',
                                            "--tc-file=../${TESTS_DIR}/platform_config/default.ini",
                                            "--tc-file=../${TESTS_DIR}/platform_config/${PLATFORM}.ini",
                                            TEST_PARAMS,
                                            '../'+TESTS_DIR+'/'+TEST_SCRIPT
                                        ]
                                    )
                                }
                            }
                        }
                    }
                }

                //--------------------------------------------------------------
                stage('archive_test_results') {
                    steps {
                        print_step_info(env.STAGE_NAME)

                        archive_as_tar_bz2(
                            TEST_RESULT_PACKAGE,
                            [
                                'test_results.xml',
                                'test-doc/',
                                'test-logs/',
                                'log-test-doc.txt',
                                'log-test-run.txt'
                            ]
                        )
                        cleanWs()
                    }
                }
            }
        }

    }

    //--------------------------------------------------------------------------
    post {
        always {
            do_notify_bitbucket()
        }
    }

}
